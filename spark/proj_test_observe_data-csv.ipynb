{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e05875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707d2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f09ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2431242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pendulum as pdl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e89877",
   "metadata": {},
   "source": [
    "# inputs\n",
    "- city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db295039",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'chicago'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8c447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_bkt = os.getenv('GCP_GCS_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3026f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_path = os.getenv('JAR_FILE_LOC')\n",
    "creds_path = '/.google/credentials/' + os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('proj_test_observe_data') \\\n",
    "    .set(\"spark.jars\", jar_path) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", creds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b43a2e",
   "metadata": {},
   "source": [
    "### Only if an existing one already runs:\n",
    "`sc.stop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624e4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/25 15:16:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a3fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hconf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hconf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hconf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hconf.set(\"fs.gs.auth.service.account.json.keyfile\", creds_path)\n",
    "hconf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e9c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081160d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4bbef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1-time sample download for pandas reading to infer schema for everything else:\n",
    "command:\n",
    "`!wget https://data.cityofchicago.org/api/views/hx8q-mf9v/rows.csv?accessType=DOWNLOAD`\n",
    "\n",
    "note: file is `Crimes_-_2012.csv`\n",
    "\n",
    "output:\n",
    "```\n",
    "--2022-10-22 08:53:42--  https://data.cityofchicago.org/api/views/hx8q-mf9v/rows.csv?accessType=DOWNLOAD\n",
    "Resolving data.cityofchicago.org (data.cityofchicago.org)... 52.206.140.205, 52.206.68.26, 52.206.140.199\n",
    "Connecting to data.cityofchicago.org (data.cityofchicago.org)|52.206.140.205|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: unspecified [text/csv]\n",
    "Saving to: ‘rows.csv?accessType=DOWNLOAD’\n",
    "\n",
    "rows.csv?accessType     [          <=>       ]  75.99M  2.54MB/s    in 28s     \n",
    "\n",
    "2022-10-22 08:54:11 (2.68 MB/s) - ‘rows.csv?accessType=DOWNLOAD’ saved [79677853]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf5a7c",
   "metadata": {},
   "source": [
    "### check count here: raw csv file\n",
    "command:\n",
    "`!wc -l rows.csv?accessType=DOWNLOAD`\n",
    "\n",
    "output:\n",
    "`336247 rows.csv?accessType=DOWNLOAD`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee772d",
   "metadata": {},
   "source": [
    "### Because of `TypeError: Can not merge type (pandas string to spark double) for 'Location Description', 'location' fields`\n",
    "Commands:\n",
    "```\n",
    "cols = ['Case Number', 'Date', 'Block', 'IUCR', 'Primary Type', 'Description', 'Arrest', 'Domestic', 'Beat', 'Ward', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Latitude', 'Longitude']\n",
    "df_pandas = pd.read_csv('rows.csv?accessType=DOWNLOAD', nrows=100, usecols=cols)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f628033",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Command:\n",
    "`spark.createDataFrame(df_pandas).schema`\n",
    "\n",
    "Output:\n",
    "```\n",
    "StructType([StructField('Case Number', StringType(), True), StructField('Date', StringType(), True), StructField('Block', StringType(), True), StructField('IUCR', StringType(), True), StructField('Primary Type', StringType(), True), StructField('Description', StringType(), True), StructField('Arrest', BooleanType(), True), StructField('Domestic', BooleanType(), True), StructField('Beat', LongType(), True), StructField('Ward', LongType(), True), StructField('FBI Code', StringType(), True), StructField('X Coordinate', DoubleType(), True), StructField('Y Coordinate', DoubleType(), True), StructField('Year', LongType(), True), StructField('Latitude', DoubleType(), True), StructField('Longitude', DoubleType(), True)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3935473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified pattern from pandas schema\n",
    "schema_template = types.StructType([\n",
    "    types.StructField('Case Number', types.StringType(), True),\n",
    "    types.StructField('Date', types.StringType(), True),\n",
    "    types.StructField('Block', types.StringType(), True),\n",
    "    types.StructField('IUCR', types.StringType(), True),\n",
    "    types.StructField('Primary Type', types.StringType(), True),\n",
    "    types.StructField('Description', types.StringType(), True),\n",
    "    types.StructField('Location Description', types.StringType(), True),\n",
    "    types.StructField('Arrest', types.BooleanType(), True),\n",
    "    types.StructField('Domestic', types.BooleanType(), True),\n",
    "    types.StructField('Beat', types.StringType(), True),\n",
    "    types.StructField('Ward', types.IntegerType(), True),\n",
    "    types.StructField('FBI Code', types.StringType(), True),\n",
    "    types.StructField('X Coordinate', types.FloatType(), True),\n",
    "    types.StructField('Y Coordinate', types.FloatType(), True),\n",
    "    types.StructField('Year', types.IntegerType(), True),\n",
    "    types.StructField('Latitude', types.FloatType(), True),\n",
    "    types.StructField('Longitude', types.FloatType(), True),\n",
    "    types.StructField('Location', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fe796",
   "metadata": {},
   "source": [
    "### Replace below with me:\n",
    "```\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema_template) \\\n",
    "    .csv(f'{gcs_bkt}/raw/{city}/')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377b7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema_template) \\\n",
    "    .csv(f'{gcs_bkt}/raw/{city}/' + 'Crimes_-_2001.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1dd45",
   "metadata": {},
   "source": [
    "### check count here: original df\n",
    "Command:\n",
    "`df_csv.count()`\n",
    "\n",
    "Output:\n",
    "`485853`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbee198",
   "metadata": {},
   "source": [
    "### check count Jan: csv df, strdate col\n",
    "Command:\n",
    "```\n",
    "df_time \\\n",
    "    .filter(F.split('Date', ' ').getItem(0).startswith('01')) \\\n",
    "    .count()\n",
    "```\n",
    "Output:\n",
    "`38114`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145cf33",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### check date Jan1: csv df, strdate col\n",
    "Command:\n",
    "```\n",
    "df_time \\\n",
    "    .filter(F.split('Date', ' ').getItem(0) == '01/01/2001') \\\n",
    "    .count()\n",
    "```\n",
    "Output:\n",
    "`1825`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b340fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dt(dt_str):\n",
    "    \"\"\"\n",
    "    parse datetime object from given date string\n",
    "    \"\"\"\n",
    "    cities = ['Chicago', 'San Francisco', 'Los Angeles', 'Austin']\n",
    "    f_cities = [c.replace(' ', '_').lower() for c in cities]\n",
    "    if city == f_cities[0]:\n",
    "        return pdl.from_format(dt_str, 'MM/DD/YYYY HH:mm:ss A')\n",
    "\n",
    "parse_dt_udf = F.udf(parse_dt, returnType=types.TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ddc3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows with null values in important columns\n",
    "df_time = df_csv \\\n",
    "    .filter(F.col('Date').isNotNull() & F.col('IUCR').isNotNull()) \\\n",
    "    .withColumn('Timestamp', parse_dt_udf(df_csv.Date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06e64bc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "years_rows = df_time.dropDuplicates(['Year']).select('Year').collect()\n",
    "years = [row.Year for row in years_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f37b74",
   "metadata": {},
   "source": [
    "### check parsed dates\n",
    "\n",
    "Command: `df_time.head(10)`\n",
    "\n",
    "Output: must contain datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9d648",
   "metadata": {},
   "source": [
    "### check count here: non-null df\n",
    "Command:\n",
    "`df_time.count()`\n",
    "\n",
    "Output:\n",
    "`485853`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd6710",
   "metadata": {},
   "source": [
    "### check parsed years\n",
    "Command:\n",
    "`print(years)`\n",
    "\n",
    "Output:\n",
    "`[2001]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55b3d7",
   "metadata": {},
   "source": [
    "### check date here: csv df, datetime col\n",
    "Command:\n",
    "```\n",
    "df_time \\\n",
    "    .filter(F.month('Timestamp') == 1) \\\n",
    "    .groupBy(F.dayofmonth('Timestamp')) \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False).show()\n",
    "```\n",
    "Output:\n",
    "```\n",
    "+---------------------+-----+\n",
    "|dayofmonth(Timestamp)|count|\n",
    "+---------------------+-----+\n",
    "|                    1| 1825|\n",
    "|                   12| 1353|\n",
    "|                   15| 1312|\n",
    "|                   13| 1311|\n",
    "|                   26| 1296|\n",
    "|                    6| 1290|\n",
    "|                   17| 1288|\n",
    "|                   18| 1278|\n",
    "|                    5| 1267|\n",
    "|                   20| 1256|\n",
    "|                   16| 1251|\n",
    "|                   23| 1250|\n",
    "|                   10| 1237|\n",
    "|                   11| 1228|\n",
    "|                   27| 1215|\n",
    "|                   19| 1214|\n",
    "|                   30| 1211|\n",
    "|                   31| 1189|\n",
    "|                    9| 1184|\n",
    "|                   25| 1183|\n",
    "+---------------------+-----+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ffcdfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_part = df_time \\\n",
    "    .repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "229018af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 239:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|dayofmonth(Timestamp)|count|\n",
      "+---------------------+-----+\n",
      "|                    1| 1825|\n",
      "|                   12| 1353|\n",
      "|                   15| 1312|\n",
      "|                   13| 1311|\n",
      "|                   26| 1296|\n",
      "|                    6| 1290|\n",
      "|                   17| 1288|\n",
      "|                   18| 1278|\n",
      "|                    5| 1267|\n",
      "|                   20| 1256|\n",
      "|                   16| 1251|\n",
      "|                   23| 1250|\n",
      "|                   10| 1237|\n",
      "|                   11| 1228|\n",
      "|                   27| 1215|\n",
      "|                   19| 1214|\n",
      "|                   30| 1211|\n",
      "|                   31| 1189|\n",
      "|                    9| 1184|\n",
      "|                   25| 1183|\n",
      "+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_time_part \\\n",
    "    .filter(F.month('Timestamp') == 1) \\\n",
    "    .groupBy(F.dayofmonth('Timestamp')) \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e641cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:============================>                            (1 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "# selected columns for analysis\n",
    "df_time_cols = ['Case Number', 'Timestamp', 'Block', 'Primary Type', 'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat', 'Latitude', 'Longitude']\n",
    "\n",
    "for year in years:\n",
    "    df = df_time.filter(F.col('Year') == year)\n",
    "    for month in range(1, 13):\n",
    "        df_filtered = df.filter(F.month('Timestamp') == month) \\\n",
    "            .select(df_time_cols) \\\n",
    "            .repartition(2)\n",
    "        for col in df_time_cols:\n",
    "            df_filtered = df_filtered.withColumnRenamed(col, col.lower().replace(' ', '_'))\n",
    "        df_filtered.write.parquet(f'{gcs_bkt}/pq/{city}/{year}/{month}', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b381b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc120d",
   "metadata": {},
   "source": [
    "### test pq accuracy\n",
    "Command:\n",
    "```\n",
    "df_pq = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet(f'{gcs_bkt}/pq/{city}/2001/1')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83986b2",
   "metadata": {},
   "source": [
    "### check count here: pq df\n",
    "Command: `df_pq.count()`\n",
    "\n",
    "Output: `38114`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b494f",
   "metadata": {},
   "source": [
    "### check date Jan: pq df\n",
    "Command:\n",
    "```\n",
    "df_pq \\\n",
    "    .filter(F.month('Timestamp') == 1).count()\n",
    "```\n",
    "Output: `1825`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682aea1",
   "metadata": {},
   "source": [
    "### check date Jan1: pq df\n",
    "Command:\n",
    "```\n",
    "df_pq \\\n",
    "    .filter(F.dayofmonth('Timestamp') == 1).count()\n",
    "```\n",
    "Output: `1825`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da80df",
   "metadata": {},
   "source": [
    "### check date here: pq df\n",
    "Command:\n",
    "```\n",
    "df_pq \\\n",
    "    .filter(F.month('Timestamp') == 1) \\\n",
    "    .groupBy(F.dayofmonth('Timestamp')) \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af3715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
