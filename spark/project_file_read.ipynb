{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e05875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707d2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f09ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2431242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pendulum as pdl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e89877",
   "metadata": {},
   "source": [
    "# inputs\n",
    "- city\n",
    "- fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db295039",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'chicago'\n",
    "fname = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8c447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_bkt = os.getenv('GCP_GCS_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3026f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_path = os.getenv('JAR_FILE_LOC')\n",
    "creds_path = '/.google/credentials/' + os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('proj_file_read') \\\n",
    "    .set(\"spark.jars\", jar_path) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", creds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b43a2e",
   "metadata": {},
   "source": [
    "### Only if an existing one already runs:\n",
    "`sc.stop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624e4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/26 07:33:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a3fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hconf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hconf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hconf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hconf.set(\"fs.gs.auth.service.account.json.keyfile\", creds_path)\n",
    "hconf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e9c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081160d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4bbef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1-time sample download for pandas reading to infer schema for everything else:\n",
    "command:\n",
    "`!wget https://data.cityofchicago.org/api/views/hx8q-mf9v/rows.csv?accessType=DOWNLOAD`\n",
    "\n",
    "note: file is `Crimes_-_2012.csv`\n",
    "\n",
    "output:\n",
    "```\n",
    "--2022-10-22 08:53:42--  https://data.cityofchicago.org/api/views/hx8q-mf9v/rows.csv?accessType=DOWNLOAD\n",
    "Resolving data.cityofchicago.org (data.cityofchicago.org)... 52.206.140.205, 52.206.68.26, 52.206.140.199\n",
    "Connecting to data.cityofchicago.org (data.cityofchicago.org)|52.206.140.205|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: unspecified [text/csv]\n",
    "Saving to: ‘rows.csv?accessType=DOWNLOAD’\n",
    "\n",
    "rows.csv?accessType     [          <=>       ]  75.99M  2.54MB/s    in 28s     \n",
    "\n",
    "2022-10-22 08:54:11 (2.68 MB/s) - ‘rows.csv?accessType=DOWNLOAD’ saved [79677853]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf5a7c",
   "metadata": {},
   "source": [
    "### check count here: raw csv file\n",
    "command:\n",
    "`!wc -l rows.csv?accessType=DOWNLOAD`\n",
    "\n",
    "output:\n",
    "`336247 rows.csv?accessType=DOWNLOAD`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee772d",
   "metadata": {},
   "source": [
    "### Because of `TypeError: Can not merge type (pandas string to spark double) for 'Location Description', 'location' fields`\n",
    "Commands:\n",
    "```\n",
    "cols = ['Case Number', 'Date', 'Block', 'IUCR', 'Primary Type', 'Description', 'Arrest', 'Domestic', 'Beat', 'Ward', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Latitude', 'Longitude']\n",
    "df_pandas = pd.read_csv('rows.csv?accessType=DOWNLOAD', nrows=100, usecols=cols)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f628033",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Command:\n",
    "`spark.createDataFrame(df_pandas).schema`\n",
    "\n",
    "Output:\n",
    "```\n",
    "StructType([StructField('Case Number', StringType(), True), StructField('Date', StringType(), True), StructField('Block', StringType(), True), StructField('IUCR', StringType(), True), StructField('Primary Type', StringType(), True), StructField('Description', StringType(), True), StructField('Arrest', BooleanType(), True), StructField('Domestic', BooleanType(), True), StructField('Beat', LongType(), True), StructField('Ward', LongType(), True), StructField('FBI Code', StringType(), True), StructField('X Coordinate', DoubleType(), True), StructField('Y Coordinate', DoubleType(), True), StructField('Year', LongType(), True), StructField('Latitude', DoubleType(), True), StructField('Longitude', DoubleType(), True)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3935473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified pattern from pandas schema\n",
    "schema_template = types.StructType([\n",
    "    types.StructField('Case Number', types.StringType(), True),\n",
    "    types.StructField('Date', types.StringType(), True),\n",
    "    types.StructField('Block', types.StringType(), True),\n",
    "    types.StructField('IUCR', types.StringType(), True),\n",
    "    types.StructField('Primary Type', types.StringType(), True),\n",
    "    types.StructField('Description', types.StringType(), True),\n",
    "    types.StructField('Location Description', types.StringType(), True),\n",
    "    types.StructField('Arrest', types.BooleanType(), True),\n",
    "    types.StructField('Domestic', types.BooleanType(), True),\n",
    "    types.StructField('Beat', types.StringType(), True),\n",
    "    types.StructField('Ward', types.IntegerType(), True),\n",
    "    types.StructField('FBI Code', types.StringType(), True),\n",
    "    types.StructField('X Coordinate', types.FloatType(), True),\n",
    "    types.StructField('Y Coordinate', types.FloatType(), True),\n",
    "    types.StructField('Year', types.IntegerType(), True),\n",
    "    types.StructField('Latitude', types.FloatType(), True),\n",
    "    types.StructField('Longitude', types.FloatType(), True),\n",
    "    types.StructField('Location', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fe796",
   "metadata": {},
   "source": [
    "### Replace below with me:\n",
    "```\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema_template) \\\n",
    "    .csv(f'{gcs_bkt}/raw/{city}/{fname}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377b7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema_template) \\\n",
    "    .csv(f'{gcs_bkt}/raw/{city}/' + 'Crimes_-_2001.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1dd45",
   "metadata": {},
   "source": [
    "### check count here: original df\n",
    "Command:\n",
    "`df_csv.count()`\n",
    "\n",
    "Output:\n",
    "`485853`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbee198",
   "metadata": {},
   "source": [
    "### check count Jan: csv df, strdate col\n",
    "Command:\n",
    "```\n",
    "df_csv \\\n",
    "    .filter(F.split('Date', ' ').getItem(0).startswith('01')) \\\n",
    "    .count()\n",
    "```\n",
    "Output:\n",
    "`38114`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145cf33",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### check date Jan1: csv df, strdate col\n",
    "Command:\n",
    "```\n",
    "df_csv \\\n",
    "    .filter(F.split('Date', ' ').getItem(0) == '01/01/2001') \\\n",
    "    .count()\n",
    "```\n",
    "Output:\n",
    "`1825`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b57c558c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "years_rows = df_csv \\\n",
    "    .select('Year') \\\n",
    "    .dropna() \\\n",
    "    .dropDuplicates(['Year']) \\\n",
    "    .collect()\n",
    "\n",
    "years = [row.Year for row in years_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685264d3",
   "metadata": {},
   "source": [
    "### check parsed years\n",
    "Command:\n",
    "`print(years)`\n",
    "\n",
    "Output:\n",
    "`[2001]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b594ca37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cols = [col.lower().replace(' ', '_') for col in df_csv.columns]\n",
    "\n",
    "for year in years:\n",
    "    df = df_csv.filter(F.col('Year') == year)\n",
    "    for month in range(1, 13):\n",
    "        df_month = df.filter(F.split('Date', ' ').getItem(0).startswith(f'{month:02}/'))\n",
    "        for i in range(len(df_csv.columns)):\n",
    "            df_month = df_month.withColumnRenamed(df_csv.columns[i], cols[i])\n",
    "        df_month \\\n",
    "            .write.parquet(f'{gcs_bkt}/pq/{city}/{year}/{month}', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
